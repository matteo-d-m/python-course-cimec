{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "from collections import namedtuple\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from sklearn.metrics import confusion_matrix as sklearn_confusion"
      ],
      "metadata": {
        "id": "RSE8FKxUMUnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cwrlCvZMLsp"
      },
      "outputs": [],
      "source": [
        "model = dict(in_channels_first=1,\n",
        "             out_channels_first=20,\n",
        "             kernel_size_first=5,\n",
        "             stride_first=2,\n",
        "             in_channels_second=20,\n",
        "             out_channels_second=20,\n",
        "             kernel_size_second=5,\n",
        "             stride_second=2,\n",
        "             in_features_first=4*4*20,\n",
        "             out_features_first=125,\n",
        "             in_features_second=125,\n",
        "             out_features_second=10)\n",
        "\n",
        "preprocessing = dict(transforms=[transforms.RandomRotation(30),\n",
        "                                 transforms.RandomVerticalFlip(p=1),\n",
        "                                 transforms.RandomHorizontalFlip(p=1),\n",
        "                                 transforms.GaussianBlur(kernel_size=5, sigma=0.2)],\n",
        "                      transformed_size=0.25)\n",
        "\n",
        "training_and_validation = dict(training_size=0.75,\n",
        "                               batch_size=15000,\n",
        "                               num_workers=2,\n",
        "                               epochs=50)\n",
        "\n",
        "model_selection = dict(dropout_p=[0.25, 0.5],\n",
        "                       learning_rate=[1e-2, 1e-3, 1e-4],\n",
        "                       weight_decay=[1e-4, 1e-5],\n",
        "                       epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_mnist_data():\n",
        "  training_dataset = torchvision.datasets.MNIST(root='classifier data',\n",
        "                                                train=True,\n",
        "                                                download=True,\n",
        "                                                transform=transforms.ToTensor())\n",
        "  test_dataset = torchvision.datasets.MNIST(root='classifier data',\n",
        "                                            train=False,\n",
        "                                            download=True,\n",
        "                                            transform=transforms.ToTensor())\n",
        "  return training_dataset, test_dataset\n",
        "\n",
        "class TransformedData(Dataset):\n",
        "  def __init__(self, list_of_labelled_images):\n",
        "    self.list_of_labelled_images = list_of_labelled_images\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.list_of_labelled_images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    sample = (self.list_of_labelled_images[idx])\n",
        "    image = sample[0]\n",
        "    label = sample[1]\n",
        "    return sample\n",
        "\n",
        "def preprocess_data(dataset, transforms_pool, subset_size):\n",
        "  \"\"\"Augments data through random affine transformations.\n",
        "\n",
        "  Parameters:\n",
        "  dataset -- a PyTorch dataset\n",
        "  transforms_pool -- the affine transforms to use (list)\n",
        "  subset_size -- fraction of data to transform (float)\n",
        "\n",
        "  Returns:\n",
        "  dataset -- the original PyTorch dataset, plus a 'subset_size'\n",
        "  percentage of transformed samples\n",
        "  \"\"\"\n",
        "  sample_and_apply = transforms.RandomApply(torch.nn.ModuleList(transforms_pool), p=1)\n",
        "  idxs_to_copy_from = np.random.randint(low=0,\n",
        "                                        high=len(dataset),\n",
        "                                        size=(int(len(dataset)*subset_size))).tolist()\n",
        "  copied_data = [dataset[idx] for idx in idxs_to_copy_from]\n",
        "  transformed_copies = [(sample_and_apply(copy[0]), copy[1]) for copy in copied_data]\n",
        "  transformed_fraction = TransformedData(transformed_copies)\n",
        "  dataset = torch.utils.data.ConcatDataset([dataset, transformed_fraction])\n",
        "  return dataset\n",
        "\n",
        "def train_validation_split(dataset, training_size):\n",
        "  \"\"\"Splits a PyTorch dataset in two unequally large subsets, then\n",
        "  instantiates one dataloader each.\n",
        "\n",
        "  Parameters:\n",
        "  dataset -- a PyTorch dataset\n",
        "  training_size -- fraction of data to use for training (float)\n",
        "\n",
        "  Returns:\n",
        "  training_loader -- PyTorch dataloader for training data\n",
        "  validation_loader -- PyTorch dataloader for validation data\n",
        "  \"\"\"\n",
        "\n",
        "  training = int(len(dataset) * training_size)\n",
        "  training_data, validation_data = random_split(dataset=dataset,\n",
        "                                                lengths=[training, len(dataset)-training],\n",
        "                                                generator=torch.Generator().manual_seed(0))\n",
        "  training_loader = DataLoader(dataset=training_data,\n",
        "                               batch_size=training_and_validation[\"batch_size\"],\n",
        "                               shuffle=True,\n",
        "                               num_workers=2,\n",
        "                               pin_memory=True)\n",
        "  validation_loader = DataLoader(dataset=validation_data,\n",
        "                                 batch_size=len(validation_data),\n",
        "                                 shuffle=False,\n",
        "                                 num_workers=2,\n",
        "                                 pin_memory=True)\n",
        "  return training_loader, validation_loader\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  \"\"\"A convolutional neural network. Check PyTorch docs\"\"\"\n",
        "\n",
        "  def __init__(self, dropout_p):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=model[\"in_channels_first\"],\n",
        "                           out_channels=model[\"out_channels_first\"],\n",
        "                           kernel_size=model[\"kernel_size_first\"],\n",
        "                           stride=model[\"stride_first\"])\n",
        "    self.conv2 = nn.Conv2d(in_channels=model[\"in_channels_second\"],\n",
        "                           out_channels=model[\"out_channels_second\"],\n",
        "                           kernel_size=model[\"kernel_size_second\"],\n",
        "                           stride=model[\"stride_second\"])\n",
        "    self.flatten = nn.Flatten(start_dim=1)\n",
        "    self.fc1 = nn.Linear(in_features=model[\"in_features_first\"],\n",
        "                         out_features=model[\"out_features_first\"])\n",
        "    self.fc2 = nn.Linear(in_features=model[\"in_features_second\"],\n",
        "                         out_features=model[\"out_features_second\"])\n",
        "    self.drop = nn.Dropout(p=dropout_p)\n",
        "    self.act = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.act(self.conv1(x))\n",
        "    x = self.act(self.conv2(x))\n",
        "    x = self.flatten(x)\n",
        "    x = self.act(self.fc1(x))\n",
        "    x = self.act(self.drop(x))\n",
        "    out = self.fc2(x)\n",
        "    return out\n",
        "\n",
        "def train_and_validate(model, device, combination, epochs, dataloaders):\n",
        "  \"\"\"Performs model training and validation.\n",
        "\n",
        "  Parameters:\n",
        "  model -- a PyTorch model instance\n",
        "  device -- where to run computations (torch device object)\n",
        "  combination -- a combination of hyperparameter values (namedtuple)\n",
        "  epochs -- number of model runs (int)\n",
        "  dataloaders -- PyTorch dataloader instances (tuple)\n",
        "  \"\"\"\n",
        "\n",
        "  cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(),\n",
        "                         lr = combination.learning_rate,\n",
        "                         weight_decay = combination.weight_decay)\n",
        "  training_loss_log = []\n",
        "  validation_loss_log = []\n",
        "  for epoch in range(epochs):\n",
        "    training_loss = []\n",
        "    model.train()\n",
        "    for batch in dataloaders[0]:\n",
        "      image = batch[0].to(device)\n",
        "      label = batch[1].to(device)\n",
        "      output = model(image)\n",
        "      loss = cross_entropy_loss(output, label)\n",
        "      model.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      loss = loss.detach().cpu().numpy()\n",
        "      training_loss.append(loss)\n",
        "    validation_loss = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for batch in dataloaders[1]:\n",
        "        image = batch[0].to(device)\n",
        "        label = batch[1].to(device)\n",
        "        output = model(image)\n",
        "        loss = cross_entropy_loss(output, label)\n",
        "        loss = loss.detach().cpu().numpy()\n",
        "        validation_loss.append(loss)\n",
        "    training_loss = np.mean(training_loss)\n",
        "    training_loss_log.append(training_loss)\n",
        "    validation_loss = np.mean(validation_loss)\n",
        "    validation_loss_log.append(validation_loss)\n",
        "    print(f\"EPOCH {epoch+1} - TRAINING LOSS: {training_loss: .2f} - VALIDATION LOSS: {validation_loss: .2f}\")\n",
        "    if epoch == epochs-1:\n",
        "      print(\"Finished\")\n",
        "  torch.save(model.state_dict(), 'model_parameters.pt')\n",
        "  return training_loss_log, validation_loss_log\n",
        "\n",
        "def combine(hyperparameters):\n",
        "  \"\"\"Constructs combinations of hyperparameter values.\n",
        "\n",
        "  Parameters:\n",
        "  hyperparameters -- map between hyperparameter names and candidate\n",
        "  values (dict, str:list)\n",
        "\n",
        "  Returns:\n",
        "  candidates -- combinations of hyperparameters values (list of namedtuples)\n",
        "  \"\"\"\n",
        "\n",
        "  candidate = namedtuple('Candidate', hyperparameters.keys())\n",
        "  candidates = []\n",
        "  for combination in product(*hyperparameters.values()):\n",
        "    candidates.append(candidate(*combination))\n",
        "  return candidates\n",
        "\n",
        "def hyperparameter_tuning(combinations, device, dataloaders):\n",
        "  \"\"\"Chooses the best combination of hyperparameters.\n",
        "\n",
        "  Parameters:\n",
        "  combinations -- hyperparameter combinations to evaluate (namedtuple)\n",
        "  device -- where to run computations (torch device object)\n",
        "  dataloaders -- PyTorch dataloader instances (tuple)\n",
        "  \"\"\"\n",
        "\n",
        "  scores = []\n",
        "  for combination in combinations:\n",
        "    model = CNN(dropout_p=combination.dropout_p)\n",
        "    model.to(device)\n",
        "    print(f\"Combination {combinations.index(combination)+1} of {len(combinations)}\")\n",
        "    score = train_and_validate(model=model,\n",
        "                               device=device,\n",
        "                               combination=combination,\n",
        "                               epochs=model_selection[\"epochs\"],\n",
        "                               dataloaders=dataloaders)\n",
        "    scores.append(score)\n",
        "  print(\"Model selection finished!\")\n",
        "  training_scores = []\n",
        "  validation_scores = []\n",
        "  for score in scores:\n",
        "    training, validation = score\n",
        "    training_scores.append(training)\n",
        "    validation_scores.append(validation)\n",
        "  least_validation_score = min(validation_scores)\n",
        "  idx = validation_scores.index(least_validation_score)\n",
        "  winner = combinations[idx]\n",
        "  return winner\n",
        "\n",
        "def plot_losses(size, losses, labels):\n",
        "  \"\"\"Draws line plots of losses (i.e., model errors) vs. epoch number.\n",
        "\n",
        "  Parameters:\n",
        "  size -- figsize (tuple)\n",
        "  losses -- the losses to draw (list)\n",
        "  labels -- the graph's lables (list)\n",
        "  \"\"\"\n",
        "\n",
        "  plt.style.use(\"dark_background\")\n",
        "  plt.figure(figsize=(size[0],size[1]))\n",
        "  plt.semilogy(losses[0], label=labels[0])\n",
        "  plt.semilogy(losses[1], label=labels[1])\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "def test(model, device, dataloader):\n",
        "  \"\"\"Evaluates the model on novel samples.\n",
        "\n",
        "  Parameters:\n",
        "  model -- a PyTorch model instance\n",
        "  device -- where to run computations (torch device object)\n",
        "  dataloader -- a PyTorch dataloader instance\n",
        "  \"\"\"\n",
        "\n",
        "  images = []\n",
        "  labels = []\n",
        "  predictions = []\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for sample in dataloader:\n",
        "      image = sample[0].to(device)\n",
        "      label = sample[1].to(device)\n",
        "      pred = model(image)\n",
        "      images.append(image)\n",
        "      labels.append(label)\n",
        "      predictions.append(pred)\n",
        "  images = torch.cat(images)\n",
        "  labels = torch.cat(labels)\n",
        "  predictions = torch.cat(predictions)\n",
        "  correct = predictions.argmax(dim=1).eq(labels).sum()\n",
        "  accuracy = correct*100/len(labels)\n",
        "  print(f\"TEST ACCURACY: {accuracy: .2f}%\")\n",
        "  return predictions\n",
        "\n",
        "def plot_confusion_matrix(true, predicted, classes):\n",
        "  \"\"\"Plots a heatmap-style confusion matrix.\n",
        "  Leverages scikit-learn's 'confusion_matrix()'\n",
        "\n",
        "  Parameters:\n",
        "  true -- ground truth labels (array-like)\n",
        "  predicted -- labels predicted by the model (array-like)\n",
        "  classes -- the number of classes in the dataset (int)\n",
        "  \"\"\"\n",
        "\n",
        "  matrix = sklearn_confusion(true, predicted)\n",
        "  plt.figure(figsize=(12,10))\n",
        "  plt.imshow(matrix, interpolation = 'nearest', cmap ='Reds')\n",
        "  matrix_cells = product(range(matrix.shape[0]), range(matrix.shape[1]))\n",
        "  for row_index, column_index in matrix_cells:\n",
        "    plt.text(x=row_index,\n",
        "             y=column_index,\n",
        "             s=matrix[row_index][column_index],\n",
        "             horizontalalignment=\"center\",\n",
        "             verticalalignment=\"center\",\n",
        "             color=\"white\" if row_index == column_index else \"black\")\n",
        "  ticks = np.arange(classes)\n",
        "  plt.xticks(ticks)\n",
        "  plt.yticks(ticks)\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.title(\"Test confusion matrix\")\n",
        "  plt.colorbar()\n",
        "  return matrix\n",
        "\n",
        "def plot_incorrect(dataset, confusion_matrix, classes):\n",
        "  \"\"\"Creates a bar chart of test mistakes per class.\n",
        "\n",
        "  Parameters:\n",
        "  dataset -- a PyTorch dataset\n",
        "  confusion_matrix -- a confusion matrix (2darray)\n",
        "  classes -- the number of classes in the dataset (int)\n",
        "  \"\"\"\n",
        "\n",
        "  bins = dataset.targets.bincount()\n",
        "  incorrect = [bins[i] - confusion_matrix[i][i] for i in range(len(bins))]\n",
        "  bars = np.arange(classes)\n",
        "  plt.figure(figsize=(12,8))\n",
        "  plt.bar(bars, incorrect)\n",
        "  plt.xticks(bars)\n",
        "  plt.xlabel(\"Class\")\n",
        "  plt.ylabel(\"Incorrectly classified count\")\n",
        "  plt.title(\"Number of mistakes per class\")\n",
        "  plt.grid(axis=\"y\")\n",
        "  plt.show()\n",
        "\n",
        "def visualize_filter(layer_filters, filter_index, reshape_dims):\n",
        "  \"\"\"Plots filter (i.e., kernel) values on a grayscale.\n",
        "\n",
        "  Parameters:\n",
        "  layer_filters -- a PyTorch tensor to index into\n",
        "  filter_index -- the index of the desired filter (int)\n",
        "  reshape_dims -- list of output dimensions for the filters tensor\n",
        "  \"\"\"\n",
        "\n",
        "  filter = layer_filters[:,filter_index,:,:]\n",
        "  filter = filter.reshape(reshape_dims[0],\n",
        "                          reshape_dims[1],\n",
        "                          reshape_dims[2],\n",
        "                          reshape_dims[3]) # 4 5 5 5\n",
        "  _, axs = plt.subplots(4,5,figsize=(12,8))\n",
        "  for i in range(filter.shape[0]):\n",
        "    for j in range(filter.shape[1]):\n",
        "      axs[i][j].imshow(filter[i][j], cmap=\"gray\")\n",
        "      axs[i][j].set_xticks([])\n",
        "      axs[i][j].set_yticks([])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "5TrnGG-vMaYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available else torch.device(\"CPU\")\n",
        "print(f\"Device is: {device}\")\n",
        "\n",
        "training_dataset, test_dataset = download_mnist_data()\n",
        "\n",
        "training_dataset = preprocess_data(dataset=training_dataset,\n",
        "                                   transforms_pool=preprocessing[\"transforms\"],\n",
        "                                   subset_size=preprocessing[\"transformed_size\"])\n",
        "\n",
        "dataloaders = train_validation_split(dataset=training_dataset,\n",
        "                                     training_size=training_and_validation[\"training_size\"])\n",
        "\n",
        "\n",
        "hyperparameters = {\n",
        "                  \"dropout_p\": model_selection[\"dropout_p\"],\n",
        "                  \"learning_rate\": model_selection[\"learning_rate\"],\n",
        "                  \"weight_decay\": model_selection[\"weight_decay\"]\n",
        "                  }\n",
        "hyperparameter_combinations = combine(hyperparameters)\n",
        "\n",
        "optimal_hyperparameters = hyperparameter_tuning(combinations=hyperparameter_combinations,\n",
        "                                                device=device,\n",
        "                                                dataloaders=dataloaders)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "model = CNN(dropout_p=optimal_hyperparameters.dropout_p)\n",
        "model.to(device)\n",
        "losses = train_and_validate(model=model,\n",
        "                            device=device,\n",
        "                            combination=optimal_hyperparameters,\n",
        "                            epochs=training_and_validation[\"epochs\"],\n",
        "                            dataloaders=dataloaders)\n",
        "\n",
        "plot_losses(size=(12,8),\n",
        "            losses=losses,\n",
        "            labels=[\"Training loss\", \"Validation loss\"])\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=len(test_dataset),\n",
        "                         shuffle=False,\n",
        "                         num_workers=0)\n",
        "\n",
        "predicted_image_labels = test(model=model,\n",
        "                              device=device,\n",
        "                              dataloader=test_loader)"
      ],
      "metadata": {
        "id": "EyeljpWNMchL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_image_labels = test_dataset.targets.cpu().numpy()\n",
        "predicted_image_labels = torch.argmax(input=predicted_image_labels,dim=1).cpu().numpy()\n",
        "confusion_matrix = plot_confusion_matrix(true=true_image_labels,\n",
        "                                         predicted=predicted_image_labels,\n",
        "                                         classes=10)"
      ],
      "metadata": {
        "id": "YN4yX3YENVho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_incorrect(dataset=test_dataset,\n",
        "               confusion_matrix=confusion_matrix,\n",
        "               classes=10)"
      ],
      "metadata": {
        "id": "VinXwbscUU3p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}